{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Protein_B-value_prediction_LSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/ibraheem-moosa/protein-bvalue-prediction/blob/torch_rnn_gpu/Protein_B_value_prediction_LSTM.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "Tnh9NCq61gBt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.0-{platform}-linux_x86_64.whl torchvision\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3q6PM2Ue16d7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2ecce93e-94c2-4a52-8200-4e68659c37ca"
      },
      "cell_type": "code",
      "source": [
        "accelerator"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cu80'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "metadata": {
        "id": "svnKgpFIFzkv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7718fa1e-248d-4c8d-f52f-5fcad649d039"
      },
      "cell_type": "code",
      "source": [
        "platform"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cp36-cp36m'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "metadata": {
        "id": "5MH5rLcyHm7u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e3ffdbd7-486c-440e-e0e4-792e5176377c"
      },
      "cell_type": "code",
      "source": [
        "torch.__version__"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.4.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "metadata": {
        "id": "vKYl33ZtPB7Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "faba9c03-c281-43ea-e735-ff4602500153"
      },
      "cell_type": "code",
      "source": [
        "torch.cuda.device_count()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "metadata": {
        "id": "NyRZTfBYPKW2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e01a3c71-af47-4af3-feaa-46322b92447e"
      },
      "cell_type": "code",
      "source": [
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla K80'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "metadata": {
        "id": "cLvxVTUs3gBt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from torch.nn.functional import relu\n",
        "from torch.nn.functional import leaky_relu\n",
        "from torch.nn.functional import dropout\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from torch.nn.utils.rnn import pad_packed_sequence\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import numpy as np\n",
        "import torch.utils.data\n",
        "from scipy.sparse import load_npz\n",
        "from numpy import load\n",
        "from bisect import bisect\n",
        "import matplotlib.pyplot as plt\n",
        "from math import sqrt\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import r2_score\n",
        "from scipy.stats import pearsonr\n",
        "import sys\n",
        "import time\n",
        "import os\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eKKTeSaJ3qxP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def summarize_tensor(tensor):\n",
        "    return torch.max(tensor).item(), torch.min(tensor).item(), torch.mean(tensor).item(), torch.std(tensor).item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xrK3hn8U3tB_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "  def close_event():\n",
        "    plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zn7gvWJz48Nr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_true_and_prediction(y_true, y_pred):\n",
        "    fig = plt.figure()\n",
        "    timer = fig.canvas.new_timer(interval=10000)\n",
        "    timer.add_callback(close_event)\n",
        "    plt.title('Bidirectional, 8 Hidden States, 2 Output Layers')\n",
        "    plt.plot(y_pred, 'y-')\n",
        "    plt.plot(y_true, 'g-')\n",
        "    timer.start()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jda8Ywnl5AMZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def summarize_nn(net):\n",
        "    print('##############################################################')\n",
        "    for name, param in net.named_parameters():\n",
        "        print('---------------------------------------------------------------')\n",
        "        print(name)\n",
        "        print(summarize_tensor(param))\n",
        "    print('##############################################################')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sDMxDO0e5EEK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_avg_pcc(net, dataset, indices):\n",
        "    pcc = []\n",
        "    for i in indices:\n",
        "        x, y = dataset[i]\n",
        "        y_pred = net.predict(x)\n",
        "        y, y_pred = y.cpu(), y_pred.cpu()\n",
        "        for j in range(x.shape[0]):\n",
        "            pcc.append(pearsonr(y_pred.numpy()[j].flatten(), y.numpy()[j].flatten())[0])\n",
        "\n",
        "    pcc = np.array(pcc)\n",
        "    pcc[np.isnan(pcc)] = 0\n",
        "    return np.mean(pcc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ITPD6SOh5GsN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def cross_validation(net, dataset, indices, k, threshold):\n",
        "    n = len(indices) // k\n",
        "    r = len(indices) - n * k\n",
        "    fold_lengths = [n + 1] * r + [n] * (k - r)\n",
        "    cumulative_fl = [0]\n",
        "    for fl in fold_lengths:\n",
        "        cumulative_fl.append(cumulative_fl[-1] + fl)\n",
        "    scores = []\n",
        "    for i in range(k):\n",
        "        print('Cross Validation Fold: {}'.format(i))\n",
        "        train_indices = []\n",
        "        validation_indices = []\n",
        "        for j in range(k):\n",
        "            if j == i:\n",
        "                validation_indices.extend(indices[cumulative_fl[j]:cumulative_fl[j+1]])\n",
        "            else:\n",
        "                train_indices.extend(indices[cumulative_fl[j]:cumulative_fl[j+1]])\n",
        "        train_pccs, validation_pccs = net.train(dataset, train_indices, validation_indices) \n",
        "        validation_pcc = max(validation_pccs)\n",
        "        scores.append(validation_pcc)\n",
        "        if validation_pcc < threshold:\n",
        "            break\n",
        "    return scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "72eBteNG5J_P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_param_config(param_grid, keys):\n",
        "    if len(keys) == 0:\n",
        "        yield None\n",
        "    else:\n",
        "        for value in param_grid[keys[0]]:\n",
        "            for rest_config in get_param_config(param_grid, keys[1:]):\n",
        "                yield keys[0], value, rest_config"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d-Le--Hm5NL9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def gridsearchcv(net, dataset, indices, k, threshold, param_grid, param_set_funcs):\n",
        "    result = []\n",
        "    num_of_params = len(param_grid)\n",
        "    for param_config in get_param_config(param_grid, list(param_grid.keys())):\n",
        "        next_param_config = param_config\n",
        "        param_config_dict = dict()\n",
        "        while True:\n",
        "            key, value, next_param_config = next_param_config\n",
        "            param_config_dict[key] = value\n",
        "            param_set_funcs[key](net, value)\n",
        "            if next_param_config is None:\n",
        "                break\n",
        "            \n",
        "        print('Running CV for params {}'.format(param_config_dict))\n",
        "        scores = cross_validation(net, dataset, indices, k, threshold)\n",
        "        mean_score = sum(scores) / len(scores)\n",
        "        print('Got score {} for params {}'.format(mean_score, param_config_dict))\n",
        "        result.append((param_config_dict, mean_score))\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jNTN-mYN5QPy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ProteinDataset:\n",
        "\n",
        "    def __init__(self, files, batch_size):\n",
        "        self._Xes = []\n",
        "        self._yes = []\n",
        "        for xf,yf in files:\n",
        "            X = torch.from_numpy(load_npz(xf).toarray()).reshape((-1, 21))\n",
        "            y = torch.from_numpy(load(yf)['y']).reshape((-1, 1))\n",
        "            assert(X.shape[0] == y.shape[0])\n",
        "            self._Xes.append(X)\n",
        "            self._yes.append(y)\n",
        "\n",
        "        self._Xes.sort(key=lambda x:x.shape[0], reverse=True)\n",
        "        self._yes.sort(key=lambda y:y.shape[0], reverse=True)\n",
        "        self._lengths = [x.shape[0] for x in self._Xes]\n",
        "\n",
        "        X_batches = []\n",
        "        y_batches = []\n",
        "        lengths = []\n",
        "        for i in range(0, len(files), batch_size):\n",
        "            stride = min(len(files) - i, batch_size)\n",
        "            lengths.append(self._lengths[i:i+stride])\n",
        "            X_batches.append(\n",
        "                        pad_sequence(self._Xes[i:i+stride], batch_first=True))\n",
        "            y_batches.append(\n",
        "                        pad_sequence(self._yes[i:i+stride], batch_first=True))\n",
        "\n",
        "        self._Xes = X_batches\n",
        "        self._yes = y_batches\n",
        "        self._lengths = lengths\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self._Xes[idx], self._yes[idx], self._lengths[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._Xes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bu3UKCuX5jdc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class FixedWidthFeedForwardNeuralNetwork(nn.Module):\n",
        "    def __init__(self, width, num_outputs, num_layers, activation):\n",
        "        super(FixedWidthFeedForwardNeuralNetwork, self).__init__()\n",
        "        self.linear_layers = [nn.Linear(width, width) for i in range(num_layers-1)]\n",
        "        self.linear_layers.append(nn.Linear(width, num_outputs))\n",
        "        self.activation = activation\n",
        "        for i in range(num_layers):\n",
        "            self.register_parameter('FF' + str(i) + '_weight_', self.linear_layers[i].weight)\n",
        "            self.register_parameter('FF' + str(i) + '_bias_', self.linear_layers[i].bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.activation(self.linear_layers[0](x))\n",
        "        for i in range(1, len(self.linear_layers) - 1):\n",
        "            out = self.activation(self.linear_layers[i](out))\n",
        "        out = self.linear_layers[-1](out)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_yuvuMsu5u03",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class RecurrentNeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size=8, output_layer_depth=1,  num_hidden_layers=1, hidden_scale=1.0, ff_scale=0.001, init_lr=1e-3, gamma=0.99, weight_decay=0.1, grad_clip=1.0):\n",
        "        super(RecurrentNeuralNetwork, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_layer_depth = output_layer_depth\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.hidden_scale = hidden_scale\n",
        "        self.ff_scale = ff_scale\n",
        "        self.init_lr = init_lr\n",
        "        self.gamma = gamma\n",
        "        self.weight_decay = weight_decay\n",
        "        self.grad_clip = grad_clip\n",
        "        self.init_layers()\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        packed_x = pack_padded_sequence(x, lengths, batch_first=True)\n",
        "        packed_out, h = self.rnn_layer(packed_x)\n",
        "        out, _ = pad_packed_sequence(packed_out, batch_first=True)\n",
        "        out = self.output_layer(out)\n",
        "        return out\n",
        "\n",
        "    def init_layers(self):\n",
        "        self.rnn_layer = nn.RNN(input_size=self.input_size, \n",
        "                                hidden_size=self.hidden_size,\n",
        "                                nonlinearity='relu',\n",
        "                                num_layers=self.num_hidden_layers,\n",
        "                                batch_first=True, \n",
        "                                bidirectional=True)\n",
        "        self.output_layer = FixedWidthFeedForwardNeuralNetwork(self.hidden_size * 2, 1, self.output_layer_depth, leaky_relu)\n",
        "        self._init_weights_()\n",
        "\n",
        "    def _init_weights_(self):\n",
        "        ff_init_method = nn.init.normal_\n",
        "        hidden_weight_init_method = nn.init.eye_\n",
        "        bias_init_method = nn.init.constant_\n",
        "        for name, param in self.rnn_layer.named_parameters():\n",
        "            if 'weight_hh' in name:\n",
        "                hidden_weight_init_method(param)\n",
        "                with torch.no_grad():\n",
        "                    param.mul_(self.hidden_scale)\n",
        "                param.requires_grad_()\n",
        "            elif 'weight_ih' in name:\n",
        "                ff_init_method(param, std=self.ff_scale)\n",
        "            else:\n",
        "                bias_init_method(param, 0)\n",
        "\n",
        "        for name, param in self.output_layer.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                ff_init_method(param, std=self.ff_scale)\n",
        "            else:\n",
        "                bias_init_method(param, 0)\n",
        "\n",
        "    def predict(self, x, lengths):\n",
        "        with torch.no_grad():\n",
        "            out = self.forward(x, lengths)\n",
        "            return out\n",
        "\n",
        "    def reset_hidden_size(self, hidden_size):\n",
        "        self.hidden_size = hidden_size\n",
        "        self.init_layers()\n",
        "\n",
        "    def reset_weight_decay(self, weight_decay):\n",
        "        self.weight_decay = weight_decay\n",
        "        self.init_layers()\n",
        "\n",
        "    def reset_gamma(self, gamma):\n",
        "        self.gamma = gamma\n",
        "        self.init_layers()\n",
        "\n",
        "    def reset_output_layer_depth(self, output_layer_depth):\n",
        "        self.output_layer_depth = output_layer_depth\n",
        "        self.init_layers()\n",
        "    \n",
        "    def reset_num_hidden_layers(self, num_hidden_layers):\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.init_layers()\n",
        "\n",
        "    def train(self, dataset, validation_dataset, model_dir=None, patience=5, warm_start_last_epoch=-1):\n",
        "        criterion = nn.MSELoss(size_average=False)\n",
        "        optimizer = optim.Adam([{'params' : self.parameters(), 'initial_lr' : self.init_lr}], \n",
        "                        lr=self.init_lr, weight_decay=self.weight_decay, amsgrad=False)\n",
        "        scheduler = optim.lr_scheduler.ExponentialLR(optimizer, self.gamma)\n",
        "        self._init_weights_()\n",
        "        num_of_batches = len(dataset)\n",
        "        validation_num_of_batches = len(validation_dataset)\n",
        "        \n",
        "        best_epoch = 0\n",
        "        best_mse = 1000.0\n",
        "        validation_mses = []\n",
        "        train_mses = []\n",
        "        for epoch in range(warm_start_last_epoch + 1, warm_start_last_epoch + 1 + 1000):\n",
        "            scheduler.step()\n",
        "            running_loss = 0.0\n",
        "            total_train_length = 0\n",
        "            for i in range(num_of_batches):\n",
        "                x, y, lengths = dataset[i]\n",
        "                total_train_length += sum(lengths)\n",
        "                optimizer.zero_grad()\n",
        "                y_pred = self.forward(x, lengths)\n",
        "                loss = criterion(y_pred, y)\n",
        "                loss.backward()\n",
        "                nn.utils.clip_grad_value_(self.parameters(), self.grad_clip)\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item()\n",
        "            running_loss /= total_train_length\n",
        "            train_mses.append(running_loss)\n",
        "\n",
        "            validation_loss = 0.0\n",
        "            total_validation_length = 0\n",
        "            for i in range(validation_num_of_batches):\n",
        "                x, y, lengths = validation_dataset[i]\n",
        "                total_validation_length += sum(lengths)\n",
        "                y_pred = self.predict(x, lengths)\n",
        "                loss = criterion(y_pred, y)\n",
        "                validation_loss += loss.item()\n",
        "            validation_loss /= total_validation_length\n",
        "            validation_mses.append(validation_loss)\n",
        "\n",
        "            if running_loss < best_mse:\n",
        "                best_mse = running_loss\n",
        "                best_epoch = epoch\n",
        "                print(best_epoch)\n",
        "            \n",
        "            print('Epoch: {0:02d} Loss: {1:.6f} Validation Loss: {2:.6f} Time: {3}'.format(\n",
        "                                    epoch, running_loss, validation_loss, time.strftime('%Y-%m-%d %H:%M:%S')))\n",
        "\n",
        "            if model_dir is not None:\n",
        "                torch.save(self.state_dict(), os.path.join(model_dir, 'net-{0:02d}'.format(epoch)))\n",
        "\n",
        "            if epoch - best_epoch == patience:\n",
        "                break\n",
        "\n",
        "        return train_mses, validation_mses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "50ia-MaL52ej",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LSTMNeuralNetwork(RecurrentNeuralNetwork):\n",
        "    def __init__(self, input_size, hidden_size=8, output_layer_depth=1,  num_hidden_layers=1, hidden_scale=1.0, ff_scale=0.001, init_lr=1e-3, gamma=0.99, weight_decay=0.1, grad_clip=1.0):\n",
        "        super(LSTMNeuralNetwork, self).__init__(input_size, hidden_size, output_layer_depth, num_hidden_layers, hidden_scale, ff_scale, init_lr, gamma, weight_decay, grad_clip)\n",
        "\n",
        "    def init_layers(self):\n",
        "        self.lstm_layer = nn.LSTM(input_size=self.input_size, \n",
        "                                hidden_size=self.hidden_size,\n",
        "                                #nonlinearity='relu',\n",
        "                                num_layers=self.num_hidden_layers,\n",
        "                                batch_first=True, \n",
        "                                bidirectional=True)\n",
        "        self.output_layer = FixedWidthFeedForwardNeuralNetwork(self.hidden_size * 2, 1, self.output_layer_depth, leaky_relu)\n",
        "        self._init_weights_()\n",
        "\n",
        "    def _init_weights_(self):\n",
        "        ff_init_method = nn.init.normal_\n",
        "        hidden_weight_init_method = nn.init.eye_\n",
        "        bias_init_method = nn.init.constant_\n",
        "        for name, param in self.lstm_layer.named_parameters():\n",
        "            if 'weight_hh' in name:\n",
        "                hidden_weight_init_method(param)\n",
        "                with torch.no_grad():\n",
        "                    param.mul_(self.hidden_scale)\n",
        "                param.requires_grad_()\n",
        "            elif 'weight_ih' in name:\n",
        "                ff_init_method(param, std=self.ff_scale)\n",
        "            else:\n",
        "                bias_init_method(param, 0)\n",
        "\n",
        "        for name, param in self.output_layer.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                ff_init_method(param, std=self.ff_scale)\n",
        "            else:\n",
        "                bias_init_method(param, 0)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        packed_x = pack_padded_sequence(x, lengths, batch_first=True)\n",
        "        packed_out, h = self.lstm_layer(packed_x)\n",
        "        out, _ = pad_packed_sequence(packed_out, batch_first=True)\n",
        "        out = self.output_layer(out)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mHgoes7c566d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QPk9-QYoA7oX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F_X09Sl2BIGS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WuFS_m5ZBOxA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "protein_list_file = drive.CreateFile({'id' : '1B8jJtU2sZGEZgI66Wn3qzjROG6Bqiusi'})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FlNH5s-yCDhH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "protein_list_file.GetContentFile('protein_list.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mkmOWsjWCMgu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rnn_inputs_compressed_file = drive.CreateFile({'id' : '1URxykv0RfgC3f0XJLteZrQ8lwTILgaZZ'})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HKLq7YIvCf6p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rnn_inputs_compressed_file.GetContentFile('rnn_inputs.tar.gz')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ow3R0LfuCp_5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!tar xzf rnn_inputs.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b6rqjx_tC0m9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3f70c11c-7bfd-4309-ade3-570085a3d424"
      },
      "cell_type": "code",
      "source": [
        "torch.set_printoptions(precision=2, linewidth=140)\n",
        "torch.manual_seed(42)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fc8e2a87f10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "metadata": {
        "id": "pJzjteirDMTX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('protein_list.txt') as protein_list_file:\n",
        "    protein_list = protein_list_file.read().split()\n",
        "    protein_list = [s.upper().strip() for s in protein_list]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ga8nNcVODdhz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_files = []\n",
        "y_files = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z5KMUb39Dknr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for protein in protein_list:\n",
        "    X_files.append(os.path.join('rnn_inputs', 'X_' + protein + '_rnn_.npz'))\n",
        "    y_files.append(os.path.join('rnn_inputs', 'y_' + protein + '_rnn_.npz'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wgkYM31Yk8ho",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vMZ5GQipxQSz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "indices = list(range(len(protein_list)))\n",
        "random.seed(42)\n",
        "random.shuffle(indices)\n",
        "train_indices = indices[:int(0.8 * len(indices))]\n",
        "test_indices = indices[int(0.8 * len(indices)):]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9KzSUhlgDxyI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "5e341be2-0da9-4534-d4c9-0c73644a2539"
      },
      "cell_type": "code",
      "source": [
        "files = list(zip(X_files, y_files))\n",
        "dataset = ProteinDataset([files[i] for i in train_indices], batch_size)\n",
        "print('Dataset init done ', len(dataset))\n",
        "test_dataset = ProteinDataset([files[i] for i in test_indices], 64)\n",
        "print('Test Dataset init done ', len(test_dataset))"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset init done  39\n",
            "Test Dataset init done  10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YlP2vjS_D4yd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "indices = list(range(len(dataset)))\n",
        "random.seed(42)\n",
        "random.shuffle(indices)\n",
        "#indices = indices[:100]\n",
        "train_indices = indices[:int(0.8 * len(indices))]\n",
        "validation_indices = indices[int(0.8 * len(indices)):]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "imeHLPwuFIn3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "warm_start_model_params = None\n",
        "warm_start_last_epoch = -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PdDCL5DPJSJV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "init_lr = 2.0 ** -10\n",
        "momentum = 0.9\n",
        "weight_decay = 1e-4\n",
        "gamma = 0.999\n",
        "hidden_size = 64\n",
        "hidden_scale = 1.0\n",
        "num_hidden_layers = 1\n",
        "output_layer_depth = 8\n",
        "ff_scale = 0.6\n",
        "grad_clip = 10.0\n",
        "nesterov = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K1EnAPy-JpZN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "net = LSTMNeuralNetwork(21,\n",
        "        hidden_size=hidden_size,\n",
        "        num_hidden_layers=num_hidden_layers,\n",
        "        output_layer_depth=output_layer_depth,\n",
        "        hidden_scale=hidden_scale, ff_scale=ff_scale, \n",
        "        init_lr=init_lr, gamma=gamma, weight_decay=weight_decay)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NMnjthyDKDF-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Omd_PAAlM4xn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fnYQBH9LJxSD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "bd99c344-5eab-4366-cd88-6a899fa98ef5"
      },
      "cell_type": "code",
      "source": [
        "net.train(dataset, test_dataset, patience=20, model_dir='models')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 00 Loss: 744324120.975313 Validation Loss: 10545276.076249 Time: 2018-09-08 06:29:00\n",
            "Epoch: 01 Loss: 5507755.982546 Validation Loss: 1400525.036001 Time: 2018-09-08 06:30:06\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}